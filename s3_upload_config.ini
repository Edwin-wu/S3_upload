
[Basic]
JobType = LOCAL_TO_S3
# 'LOCAL_TO_S3' | 'S3_TO_S3' | 'ALIOSS_TO_S3'

DesBucket = risentest
# Destination S3 bucket name
# 目标文件bucket, type = str

S3Prefix = my-prefix-2
# S3_TO_S3 mode Src. S3 Prefix, and same as Des. S3 Prefix; LOCAL_TO_S3 mode, this is Des. S3 Prefix.
# S3_TO_S3 源S3的Prefix(与目标S3一致)，LOCAL_TO_S3 则为目标S3的Prefix, type = str

SrcFileIndex = *
# Specify the file name to upload. Wildcard "*" to upload all.
# 指定要上传的文件的文件名, type = str，Upload全部文件则用 "*"

SrcFileAggreRatio = 40
# Specify the file  aggregation ratio for image file.
# 制定上传时文件的压缩比，type = int, 平均文件大小2M，聚合数目默认为40个文件聚合成一个大文件

LocalTempPath = /Users/administrator/Documents/tempfolder
# Specify the local temple path for zip file
# 设置本地临时目录用于存放聚合后的文件，注意磁盘空间

LocalFolderSize = 10
#单位为G，本地临时文件夹磁盘空间大小，请设置为10G以上，若由于上传速度低于打包速度，空间利用率接近50%以上，则主进程sleep时间增加，等待上传进程

DesProfileName = default
# Profile name config in ~/.aws credentials. It is the destination account profile.
# 在~/.aws 中配置的能访问目标S3的 profile name

ifRecoverFromLast = True
# 'False' | 'True'
# Default is False which means fresh start, you can set to True if this is a reconnect and upload.
# 如果是因为断电等异常中断的上传，可以通过设置该参数和 LastZipIndexFile 参数恢复上一次路径的上传工作

LastZipIndexFile = 2020-10-11-18-28-19-list.xlsx
# 恢复上传情况下，上一次执行产生的压缩文件和原始文件对照表
# 从文件最后一个压缩包开始打包流程

[LOCAL_TO_S3]
SrcDir = /Users/administrator/Documents/uploadtest
# Source file directory. It is useless in S3_TO_S3 mode
# 原文件本地存放目录，S3_TO_S3 则该字段无效 type = str

[Advanced]

ChunkSize = 100
# File chunksize, unit MBytes, not less than 5MB. Single file parts number < 10,000, limited by S3 mulitpart upload API. The application will auto change it adapting to file size, you don't need to change it.
# 文件分片大小，单位为MB，不小于5M，单文件分片总数不能超过10000, 所以程序会根据文件大小自动调整该值，你一般无需调整。type = int

MaxRetry = 20
# Max retry times while S3 API call fail.
# S3 API call 失败，最大重试次数, type = int

MaxThread = 2
# 进程数目，主要是利用CPU多核提高上传速度
# Max threads for ONE file.
# 单文件同时上传的进程数量, type = int

ifDeleteTempZipfile = True
# 聚合后的临时文件是否在上传结束后进行删除，删除前要double check一下文件大小和MD5是否一致。


MaxParallelFile = 5
# Max paralle running file, i.e. concurrency threads = MaxParallelFile * MaxThread
# 并行操作文件数量, type = int, 即同时并发的进程数 = MaxParallelFile * MaxThread

StorageClass = DEEP_ARCHIVE
# 'STANDARD'|'REDUCED_REDUNDANCY'|'STANDARD_IA'|'ONEZONE_IA'|'INTELLIGENT_TIERING'|'GLACIER'|'DEEP_ARCHIVE'

ifVerifyMD5 = False
# Practice for twice MD5 for whole file.
# If True, then after merge file, will do the second time of Etag MD5 for the whole file.
# In S3_TO_S3 mode, this True will force to re-download all parts while break-point resume for calculating MD5, but not reupload the parts which already uploaded.
# This switch will not affect the MD5 verification of every part upload, even False, it still verify very part's MD5.
# 是否做这个文件的二次的MD5校验
# 为True则一个文件完成上传合并分片之后再次进行整个文件的ETag校验MD5。
# 对于 S3_TO_S3，该开关True会在断点续传的时候重新下载所有已传过的分片来计算MD5。
# 该开关不影响每个分片上传时候的校验，即使为False也会校验每个分片MD5。

DontAskMeToClean = False
# If True: While there is unfinished upload, it will not ask you to clean the unfinished parts on Des. S3 or not. It will move on and resume break-point upload.
# If True: 遇到存在现有的未完成upload时，不再询问是否Clean，默认不Clean，自动续传

LoggingLevel = INFO
# 'WARNING' | 'INFO' | 'DEBUG'
